{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006edd18",
   "metadata": {},
   "source": [
    "# JAX & FLAX\n",
    "\n",
    "> **Author**: Gustavo L. F. Walbon / **Date**: July 2022.\n",
    "\n",
    "In this work we will have the inference of resnet18 with Jax engines.\n",
    "\n",
    "## Objective\n",
    "_In the fourth project you will have to implement the ResNet-18 model using JAX and Flax libraries for inference only. The FLAX library, discussed in the second lecture, provides primitives to stack multiple kinds of layers in order to form a neural network architecture._\n",
    "\n",
    "_After you have implemented the model and loaded the weights, it is time to test your code on a few test images. You may use the images provided in [5](https://github.com/MO436-MC934/work). Finally, you job is to obtain the maximum amount of performance improvement from your network using JAX transformations: jit, vmap, pmap, etc. You may use your CPU, GPU or TPU (in Google Collab). In the end, you should write a report (PDF) describing how you implemented your model, which transformations you applied and why. If you could not apply some transformations, discuss the problems you found while trying to use it. Finally, do a performance analysis showing how fast your model has become compared to the non-transformed model (tables and graphs are welcome)._\n",
    "\n",
    "_Reference: https://github.com/MO436-MC934/notebooks/wiki/5.JAX-Library_\n",
    "\n",
    "### Loading Resnet18\n",
    "Pre-trained network is used to see the weights of kernel and bias of Resnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d8f81fbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Tensor: 'resnetv15_conv0_weight'                      shape=(64, 3, 7, 7)\n",
      "- Tensor: 'resnetv15_stage1_conv0_weight'               shape=(64, 64, 3, 3)\n",
      "- Tensor: 'resnetv15_stage1_conv1_weight'               shape=(64, 64, 3, 3)\n",
      "- Tensor: 'resnetv15_stage1_conv2_weight'               shape=(64, 64, 3, 3)\n",
      "- Tensor: 'resnetv15_stage1_conv3_weight'               shape=(64, 64, 3, 3)\n",
      "- Tensor: 'resnetv15_stage2_conv2_weight'               shape=(128, 64, 1, 1)\n",
      "- Tensor: 'resnetv15_stage2_conv0_weight'               shape=(128, 64, 3, 3)\n",
      "- Tensor: 'resnetv15_stage2_conv1_weight'               shape=(128, 128, 3, 3)\n",
      "- Tensor: 'resnetv15_stage2_conv3_weight'               shape=(128, 128, 3, 3)\n",
      "- Tensor: 'resnetv15_stage2_conv4_weight'               shape=(128, 128, 3, 3)\n",
      "- Tensor: 'resnetv15_stage3_conv2_weight'               shape=(256, 128, 1, 1)\n",
      "- Tensor: 'resnetv15_stage3_conv0_weight'               shape=(256, 128, 3, 3)\n",
      "- Tensor: 'resnetv15_stage3_conv1_weight'               shape=(256, 256, 3, 3)\n",
      "- Tensor: 'resnetv15_stage3_conv3_weight'               shape=(256, 256, 3, 3)\n",
      "- Tensor: 'resnetv15_stage3_conv4_weight'               shape=(256, 256, 3, 3)\n",
      "- Tensor: 'resnetv15_stage4_conv2_weight'               shape=(512, 256, 1, 1)\n",
      "- Tensor: 'resnetv15_stage4_conv0_weight'               shape=(512, 256, 3, 3)\n",
      "- Tensor: 'resnetv15_stage4_conv1_weight'               shape=(512, 512, 3, 3)\n",
      "- Tensor: 'resnetv15_stage4_conv3_weight'               shape=(512, 512, 3, 3)\n",
      "- Tensor: 'resnetv15_stage4_conv4_weight'               shape=(512, 512, 3, 3)\n",
      "- Tensor: 'resnetv15_dense0_weight'                     shape=(1000, 512)\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx import numpy_helper\n",
    "\n",
    "model = onnx.load(\"resnet18.onnx\")\n",
    "\n",
    "for initializer in model.graph.initializer:\n",
    "    array = numpy_helper.to_array(initializer)\n",
    "    if 'weight' in initializer.name:\n",
    "        print(f\"- Tensor: {initializer.name!r:45} shape={array.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0cbc98",
   "metadata": {},
   "source": [
    "With that the next step is to create the dicionary with the same shape to be used in the interference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1913ea8b",
   "metadata": {},
   "source": [
    "### Creating de CNN\n",
    "\n",
    "Resnet models has the format NCHW(Batch size, Channels, Height, Width), so that means the eg. (64, 3, 7, 7) refers to N=64, Channels=3, Height=7 and Width=7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dedda06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a787263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "import optax\n",
    "\n",
    "# We need this to hold the training state\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "from typing import Any, Callable, Sequence, Tuple\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    \"\"\"ResNet block.\"\"\"\n",
    "    filters: int\n",
    "    conv: Any\n",
    "    norm: Any\n",
    "    act: Callable\n",
    "    strides: Tuple[int, int] = (1, 1)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x,):\n",
    "        residual = x\n",
    "        y = self.conv(self.filters, (3, 3), self.strides)(x)\n",
    "        y = self.norm()(y)\n",
    "        y = self.act(y)\n",
    "        y = self.conv(self.filters, (3, 3))(y)\n",
    "        y = self.norm(scale_init=nn.initializers.zeros)(y)\n",
    "\n",
    "        if residual.shape != y.shape:\n",
    "            residual = self.conv(self.filters, (1, 1),\n",
    "                               self.strides)(residual)\n",
    "            residual = self.norm()(residual)\n",
    "\n",
    "        return self.act(residual + y)\n",
    "\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    num_filters: int = 64\n",
    "    dtype = jnp.float32\n",
    "    conv = nn.Conv\n",
    "    norm = nn.BatchNorm\n",
    "    act = nn.relu\n",
    "    \n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        conv = partial(nn.Conv,\n",
    "                    use_bias=False,\n",
    "                    dtype=self.dtype\n",
    "                   )\n",
    "        norm = partial(nn.BatchNorm,\n",
    "                       use_running_average=False,\n",
    "                       momentum=0.9,\n",
    "                       epsilon=1e-5,\n",
    "                       dtype=self.dtype\n",
    "                      )\n",
    "        x = norm(name='batchnorm')(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.max_pool(x, #Input\n",
    "                        window_shape=(3,3),\n",
    "                        strides=(2,2),\n",
    "                        padding='SAME')\n",
    "        \n",
    "        # -- Stage 1 weights (64, 64, 3, 3)--\n",
    "        x = ResNetBlock(64, conv=conv,norm=norm, strides=(1,1), act=nn.relu)(x)\n",
    "        x = ResNetBlock(64, conv=conv,norm=norm, strides=(1,1), act=nn.relu)(x)\n",
    "        x = ResNetBlock(64, conv=conv,norm=norm, strides=(1,1), act=nn.relu)(x)\n",
    "        x = ResNetBlock(64, conv=conv,norm=norm, strides=(1,1), act=nn.relu)(x)\n",
    "        # -- Stage 2 weights (128, 64, 1, 1)--\n",
    "        x = ResNetBlock(128, conv=conv,norm=norm, strides=(2,2), act=nn.relu)(x)\n",
    "        x = ResNetBlock(128, conv=conv,norm=norm, strides=(1,1), act=nn.relu)(x)\n",
    "        x = ResNetBlock(128, conv=conv,norm=norm, strides=(1,1), act=nn.relu)(x)\n",
    "        x = ResNetBlock(128, conv=conv,norm=norm, strides=(1,1), act=nn.relu)(x)\n",
    "        # -- Stage 3 weights (256, 128, 1, 1)--\n",
    "        x = ResNetBlock(256, conv=conv,norm=norm, strides=(2,2), act=nn.relu)(x)\n",
    "        x = ResNetBlock(256, conv=conv,norm=norm, strides=(1,1), act=nn.relu)(x)\n",
    "        x = ResNetBlock(256, conv=conv,norm=norm, strides=(1,1), act=nn.relu)(x)\n",
    "        x = ResNetBlock(256, conv=conv,norm=norm, strides=(1,1), act=nn.relu)(x)\n",
    "        # -- Stage 4 weights (512, 256, 1, 1)--\n",
    "        x = ResNetBlock(512, conv=conv,norm=norm, strides=(2,2), act=nn.relu)(x)\n",
    "        x = ResNetBlock(512, conv=conv,norm=norm, strides=(1,1), act=nn.relu)(x)\n",
    "        x = ResNetBlock(512, conv=conv,norm=norm, strides=(1,1), act=nn.relu)(x)\n",
    "        x = ResNetBlock(512, conv=conv,norm=norm, strides=(1,1), act=nn.relu)(x)\n",
    "        \n",
    "        x = jnp.mean(x, axis=(1,2))\n",
    "        x = nn.Dense(512, dtype=self.dtype)(x)\n",
    "        x = jnp.asarray(x, self.dtype)\n",
    "     \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab3e7f6",
   "metadata": {},
   "source": [
    "The function `__call__` was overwritten to be used internally by the Module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208ad9da",
   "metadata": {},
   "source": [
    "### Evaluation Function\n",
    "Next we define a function to evaluate the entire model and summarize the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0167948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(params, test_ds):\n",
    "    metrics = eval_step(params, test_ds)\n",
    "    metrics = jax.device_get(metrics)\n",
    "    summary = jax.tree_map(lambda x: x.item(), metrics)\n",
    "    return summary['loss'], summary['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37d55f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(*, y_pred, y_label):\n",
    "    \"\"\"Compute cross-entropy loss given the activations and ground-truth labels.\"\"\"\n",
    "    # Labels are numeric, convert them to one hot\n",
    "    y_label_one_hot = jax.nn.one_hot(y_label, num_classes=10)\n",
    "    # Return cross-entropy loss\n",
    "    return -jnp.mean(jnp.sum(y_label_one_hot * y_pred, axis=-1))\n",
    "\n",
    "def compute_metrics(*, y_pred, y_label):\n",
    "    \"\"\"Compute metrics for a subset of data.\"\"\"\n",
    "    return {\n",
    "        'loss': cross_entropy_loss(y_pred=y_pred, y_label=y_label),\n",
    "        'accuracy': jnp.mean(jnp.argmax(y_pred, -1) == y_label),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7161b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 19:25:54.422226: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    Dense_0: {\n",
      "        bias: (512,),\n",
      "        kernel: (512, 512),\n",
      "    },\n",
      "    ResNetBlock_0: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (64,),\n",
      "            scale: (64,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (64,),\n",
      "            scale: (64,),\n",
      "        },\n",
      "        BatchNorm_2: {\n",
      "            bias: (64,),\n",
      "            scale: (64,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 1, 64),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 64, 64),\n",
      "        },\n",
      "        Conv_2: {\n",
      "            kernel: (1, 1, 1, 64),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_1: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (64,),\n",
      "            scale: (64,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (64,),\n",
      "            scale: (64,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 64, 64),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 64, 64),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_10: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (256,),\n",
      "            scale: (256,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (256,),\n",
      "            scale: (256,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 256, 256),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 256, 256),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_11: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (256,),\n",
      "            scale: (256,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (256,),\n",
      "            scale: (256,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 256, 256),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 256, 256),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_12: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (512,),\n",
      "            scale: (512,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (512,),\n",
      "            scale: (512,),\n",
      "        },\n",
      "        BatchNorm_2: {\n",
      "            bias: (512,),\n",
      "            scale: (512,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 256, 512),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 512, 512),\n",
      "        },\n",
      "        Conv_2: {\n",
      "            kernel: (1, 1, 256, 512),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_13: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (512,),\n",
      "            scale: (512,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (512,),\n",
      "            scale: (512,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 512, 512),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 512, 512),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_14: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (512,),\n",
      "            scale: (512,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (512,),\n",
      "            scale: (512,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 512, 512),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 512, 512),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_15: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (512,),\n",
      "            scale: (512,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (512,),\n",
      "            scale: (512,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 512, 512),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 512, 512),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_2: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (64,),\n",
      "            scale: (64,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (64,),\n",
      "            scale: (64,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 64, 64),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 64, 64),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_3: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (64,),\n",
      "            scale: (64,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (64,),\n",
      "            scale: (64,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 64, 64),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 64, 64),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_4: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (128,),\n",
      "            scale: (128,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (128,),\n",
      "            scale: (128,),\n",
      "        },\n",
      "        BatchNorm_2: {\n",
      "            bias: (128,),\n",
      "            scale: (128,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 64, 128),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 128, 128),\n",
      "        },\n",
      "        Conv_2: {\n",
      "            kernel: (1, 1, 64, 128),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_5: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (128,),\n",
      "            scale: (128,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (128,),\n",
      "            scale: (128,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 128, 128),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 128, 128),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_6: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (128,),\n",
      "            scale: (128,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (128,),\n",
      "            scale: (128,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 128, 128),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 128, 128),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_7: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (128,),\n",
      "            scale: (128,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (128,),\n",
      "            scale: (128,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 128, 128),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 128, 128),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_8: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (256,),\n",
      "            scale: (256,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (256,),\n",
      "            scale: (256,),\n",
      "        },\n",
      "        BatchNorm_2: {\n",
      "            bias: (256,),\n",
      "            scale: (256,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 128, 256),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 256, 256),\n",
      "        },\n",
      "        Conv_2: {\n",
      "            kernel: (1, 1, 128, 256),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_9: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (256,),\n",
      "            scale: (256,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (256,),\n",
      "            scale: (256,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 256, 256),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 256, 256),\n",
      "        },\n",
      "    },\n",
      "    batchnorm: {\n",
      "        bias: (1,),\n",
      "        scale: (1,),\n",
      "    },\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Create random number generator\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "\n",
    "# Create train state\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "state = create_train_state(init_rng, learning_rate, momentum)\n",
    "\n",
    "print(jax.tree_map(lambda x: x.shape, state.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c902a",
   "metadata": {},
   "source": [
    "### Load MNIST Dataset\n",
    "We will use Tensorflow to load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff629202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def load_mnist():\n",
    "    \"\"\"Load MNIST dataset using Tensorflow.\"\"\"\n",
    "    \n",
    "    # Download data\n",
    "    ds_builder = tfds.builder('mnist')\n",
    "    ds_builder.download_and_prepare()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "    test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
    "    \n",
    "    # Normalize images between 0.0 and 1.0\n",
    "    train_ds['image'] = jnp.float32(train_ds['image']) / 255.\n",
    "    test_ds['image'] = jnp.float32(test_ds['image']) / 255.\n",
    "    \n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51e79bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 19:27:28.889113: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: {'image': ((60000, 28, 28, 1), dtype('float32')), 'label': ((60000,), dtype('int64'))}\n",
      " Test dataset: {'image': ((10000, 28, 28, 1), dtype('float32')), 'label': ((10000,), dtype('int64'))}\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_ds, test_ds = load_mnist()\n",
    "\n",
    "# Get overview of both datasets\n",
    "print(\"Train dataset:\", jax.tree_map(lambda x: (x.shape, x.dtype), train_ds))\n",
    "print(\" Test dataset:\", jax.tree_map(lambda x: (x.shape, x.dtype), test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d755e8",
   "metadata": {},
   "source": [
    "## Training Step\n",
    "\n",
    "Now it is time to define a training step on a single batch of images. This function will be executed thousands of times during the training loop, therefore it is interesting to JIT compile it. Do remember that by JIT compiling this function we are also compling everything that is called by `train_step`, in particular, `CNN.apply` and `loss_fn` will also be compiled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "089dcc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, learning_rate, momentum):\n",
    "    # Instantiate model\n",
    "    cnn = ResNet18()\n",
    "    # Initialize parameters\n",
    "    params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
    "    # Instantiate optimizer\n",
    "    tx = optax.sgd(learning_rate, momentum)\n",
    "    # Instantiate train state\n",
    "    return TrainState.create(apply_fn=cnn.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0996cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"Perform a training step on a batch of images and update state.\"\"\"\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        \"\"\"Feed forward current batch, returns loss and\n",
    "        activations of the last layer.\"\"\"\n",
    "        \n",
    "        # Apply (i.e. predict) the model on a batch of images\n",
    "        y_pred = ResNet18().apply({'params': params}, batch['image'])\n",
    "        # Compute loss from the predictions\n",
    "        loss = cross_entropy_loss(y_pred=y_pred, y_label=batch['label'])\n",
    "        \n",
    "        return loss, y_pred\n",
    "    \n",
    "    # Compute gradient (and value) of loss function\n",
    "    (_, y_pred), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    # Update model parameters using gradients (!)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    # Compute metrics for this batch\n",
    "    metrics = compute_metrics(y_pred=y_pred, y_label=batch['label'])\n",
    "    \n",
    "    return state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "380caf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_step(params, batch):\n",
    "    \"\"\"Evaluate model on a testing batch.\"\"\"\n",
    "    y_pred = ResNet18().apply({'params': params}, batch['image'])\n",
    "    return compute_metrics(y_pred=y_pred, y_label=batch['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d9f1239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state, train_ds, batch_size, epoch, rng):\n",
    "    \"\"\"Train the model for an entire epoch.\"\"\"\n",
    "    \n",
    "    train_ds_size = len(train_ds['image'])\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "    \n",
    "    # Shuffle the dataset and divide it into mini-batches\n",
    "    perms = jax.random.permutation(rng, train_ds_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size]\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "    \n",
    "    # Perform a train step in each batch and save metrics\n",
    "    batch_metrics = []\n",
    "    for perm in perms:\n",
    "        batch = {k: v[perm, ...] for k, v in train_ds.items()}\n",
    "        state, metrics = train_step(state, batch)\n",
    "        batch_metrics.append(metrics)\n",
    "    \n",
    "    # Average metrics for each batch\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "        k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "        for k in batch_metrics_np[0]\n",
    "    }\n",
    "    \n",
    "    # Retrieve loss and accuracy\n",
    "    loss = epoch_metrics_np['loss']\n",
    "    acc = epoch_metrics_np['accuracy'] * 100\n",
    "    \n",
    "    return state, loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1952f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(params, test_ds):\n",
    "    metrics = eval_step(params, test_ds)\n",
    "    metrics = jax.device_get(metrics)\n",
    "    summary = jax.tree_map(lambda x: x.item(), metrics)\n",
    "    return summary['loss'], summary['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75d99411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    Dense_0: {\n",
      "        bias: (512,),\n",
      "        kernel: (512, 512),\n",
      "    },\n",
      "    ResNetBlock_0: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (64,),\n",
      "            scale: (64,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (64,),\n",
      "            scale: (64,),\n",
      "        },\n",
      "        BatchNorm_2: {\n",
      "            bias: (64,),\n",
      "            scale: (64,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 1, 64),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 64, 64),\n",
      "        },\n",
      "        Conv_2: {\n",
      "            kernel: (1, 1, 1, 64),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_1: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (64,),\n",
      "            scale: (64,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (64,),\n",
      "            scale: (64,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 64, 64),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 64, 64),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_10: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (256,),\n",
      "            scale: (256,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (256,),\n",
      "            scale: (256,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 256, 256),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 256, 256),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_11: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (256,),\n",
      "            scale: (256,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (256,),\n",
      "            scale: (256,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 256, 256),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 256, 256),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_12: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (512,),\n",
      "            scale: (512,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (512,),\n",
      "            scale: (512,),\n",
      "        },\n",
      "        BatchNorm_2: {\n",
      "            bias: (512,),\n",
      "            scale: (512,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 256, 512),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 512, 512),\n",
      "        },\n",
      "        Conv_2: {\n",
      "            kernel: (1, 1, 256, 512),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_13: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (512,),\n",
      "            scale: (512,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (512,),\n",
      "            scale: (512,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 512, 512),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 512, 512),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_14: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (512,),\n",
      "            scale: (512,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (512,),\n",
      "            scale: (512,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 512, 512),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 512, 512),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_15: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (512,),\n",
      "            scale: (512,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (512,),\n",
      "            scale: (512,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 512, 512),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 512, 512),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_2: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (64,),\n",
      "            scale: (64,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (64,),\n",
      "            scale: (64,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 64, 64),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 64, 64),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_3: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (64,),\n",
      "            scale: (64,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (64,),\n",
      "            scale: (64,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 64, 64),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 64, 64),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_4: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (128,),\n",
      "            scale: (128,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (128,),\n",
      "            scale: (128,),\n",
      "        },\n",
      "        BatchNorm_2: {\n",
      "            bias: (128,),\n",
      "            scale: (128,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 64, 128),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 128, 128),\n",
      "        },\n",
      "        Conv_2: {\n",
      "            kernel: (1, 1, 64, 128),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_5: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (128,),\n",
      "            scale: (128,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (128,),\n",
      "            scale: (128,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 128, 128),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 128, 128),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_6: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (128,),\n",
      "            scale: (128,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (128,),\n",
      "            scale: (128,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 128, 128),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 128, 128),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_7: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (128,),\n",
      "            scale: (128,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (128,),\n",
      "            scale: (128,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 128, 128),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 128, 128),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_8: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (256,),\n",
      "            scale: (256,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (256,),\n",
      "            scale: (256,),\n",
      "        },\n",
      "        BatchNorm_2: {\n",
      "            bias: (256,),\n",
      "            scale: (256,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 128, 256),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 256, 256),\n",
      "        },\n",
      "        Conv_2: {\n",
      "            kernel: (1, 1, 128, 256),\n",
      "        },\n",
      "    },\n",
      "    ResNetBlock_9: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (256,),\n",
      "            scale: (256,),\n",
      "        },\n",
      "        BatchNorm_1: {\n",
      "            bias: (256,),\n",
      "            scale: (256,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            kernel: (3, 3, 256, 256),\n",
      "        },\n",
      "        Conv_1: {\n",
      "            kernel: (3, 3, 256, 256),\n",
      "        },\n",
      "    },\n",
      "    batchnorm: {\n",
      "        bias: (1,),\n",
      "        scale: (1,),\n",
      "    },\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Create random number generator\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "\n",
    "# Create train state\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "state = create_train_state(init_rng, learning_rate, momentum)\n",
    "\n",
    "print(jax.tree_map(lambda x: x.shape, state.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb893a96",
   "metadata": {},
   "outputs": [
    {
     "ename": "ScopeCollectionNotFound",
     "evalue": "Tried to access \"mean\" from collection \"batch_stats\" in \"/batchnorm\" but the collection is empty. (https://flax.readthedocs.io/en/latest/flax.errors.html#flax.errors.ScopeCollectionNotFound)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScopeCollectionNotFound\u001b[0m                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      5\u001b[0m     rng, input_rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng)\n\u001b[0;32m----> 6\u001b[0m     state, loss, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_rng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m eval_model(state\u001b[38;5;241m.\u001b[39mparams, test_ds)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -----------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(state, train_ds, batch_size, epoch, rng)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m perm \u001b[38;5;129;01min\u001b[39;00m perms:\n\u001b[1;32m     15\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v[perm, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m train_ds\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 16\u001b[0m     state, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     batch_metrics\u001b[38;5;241m.\u001b[39mappend(metrics)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Average metrics for each batch\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(state, batch)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, y_pred\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Compute gradient (and value) of loss function\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m (_, y_pred), grads \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Update model parameters using gradients (!)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mapply_gradients(grads\u001b[38;5;241m=\u001b[39mgrads)\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mtrain_step.<locals>.loss_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"Feed forward current batch, returns loss and\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mactivations of the last layer.\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Apply (i.e. predict) the model on a batch of images\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mResNet18\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Compute loss from the predictions\u001b[39;00m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m cross_entropy_loss(y_pred\u001b[38;5;241m=\u001b[39my_pred, y_label\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "    \u001b[0;31m[... skipping hidden 5 frame]\u001b[0m\n",
      "File \u001b[0;32m/usr/lib64/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mResNet18.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m conv \u001b[38;5;241m=\u001b[39m partial(nn\u001b[38;5;241m.\u001b[39mConv,\n\u001b[1;32m     45\u001b[0m             use_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m             dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m     47\u001b[0m            )\n\u001b[1;32m     48\u001b[0m norm \u001b[38;5;241m=\u001b[39m partial(nn\u001b[38;5;241m.\u001b[39mBatchNorm,\n\u001b[1;32m     49\u001b[0m                use_running_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     50\u001b[0m                momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     51\u001b[0m                epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m,\n\u001b[1;32m     52\u001b[0m                dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m     53\u001b[0m               )\n\u001b[0;32m---> 54\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatchnorm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     56\u001b[0m x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mmax_pool(x, \u001b[38;5;66;03m#Input\u001b[39;00m\n\u001b[1;32m     57\u001b[0m                 window_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m     58\u001b[0m                 strides\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     59\u001b[0m                 padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAME\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/usr/lib64/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flax/linen/normalization.py:251\u001b[0m, in \u001b[0;36mBatchNorm.__call__\u001b[0;34m(self, x, use_running_average)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# see NOTE above on initialization behavior\u001b[39;00m\n\u001b[1;32m    249\u001b[0m initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_mutable_collection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 251\u001b[0m ra_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_stats\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mfeature_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m ra_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_stats\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    255\u001b[0m                        \u001b[38;5;28;01mlambda\u001b[39;00m s: jnp\u001b[38;5;241m.\u001b[39mones(s, jnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m    256\u001b[0m                        feature_shape)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_running_average:\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flax/core/scope.py:709\u001b[0m, in \u001b[0;36mScope.variable\u001b[0;34m(self, col, name, init_fn, *init_args)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_mutable_collection(col) \u001b[38;5;129;01mor\u001b[39;00m init_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    708\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_collection_empty(col):\n\u001b[0;32m--> 709\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeCollectionNotFound(col, name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text)\n\u001b[1;32m    710\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeVariableNotFoundError(name, col, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text)\n\u001b[1;32m    711\u001b[0m init_value \u001b[38;5;241m=\u001b[39m init_fn(\u001b[38;5;241m*\u001b[39minit_args)\n",
      "\u001b[0;31mScopeCollectionNotFound\u001b[0m: Tried to access \"mean\" from collection \"batch_stats\" in \"/batchnorm\" but the collection is empty. (https://flax.readthedocs.io/en/latest/flax.errors.html#flax.errors.ScopeCollectionNotFound)"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    state, loss, acc = train_epoch(state, train_ds, batch_size, epoch, input_rng)\n",
    "    test_loss, test_acc = eval_model(state.params, test_ds)\n",
    "    \n",
    "    print(f\"EPOCH {epoch:02} -----------------------------\")\n",
    "    print(f\">> Train:  loss={loss:.8f}  acc={acc:.2f}%\")\n",
    "    print(f\">>  Test:  loss={test_loss:.8f}  acc={test_acc*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
